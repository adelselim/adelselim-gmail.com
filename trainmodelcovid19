import pandas as pd
from sklearn import ensemble
from sklearn.externals import joblib
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error
import numpy as np
#from sklearn.impute import SimpleImputer
#imp = SimpleImputer(missing_values=np.nan, strategy='mean')
#imp.fit([[1, 2], [np.nan, 3], [7, 6]])
#SimpleImputer()

# Load the data set
df = pd.read_csv("COVID19update5.csv")
new_df =df.dropna()

# Remove the fields from the data set that we don't want to include in our model
#del df['house_number']
#del df['unit_number']
#del df['street_name']
#del df['zip_code']

# Replace categorical data with one-hot encoded data
#features_df =  pd.get_dummies(df, columns=['garage_type', 'city'])

# Remove the sale price from the feature data

features_df = pd.get_dummies(new_df, columns=['gender','country','location'])
del features_df['death']
# Create the X and y arrays


X = features_df.to_numpy()
#X = [[np.nan, 2], [6, np.nan], [7, 6]]
#print(imp.transform(X))
y = new_df['death'].to_numpy()


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)
model = ensemble.GradientBoostingRegressor(
    n_estimators=1000,
    learning_rate=0.1,
    max_depth=6,
    min_samples_leaf=9,
    max_features=0.1,
    loss='huber',
    random_state=0
)
model.fit(X_train, y_train)

# Save the trained model to a file so we can use it in other programs
joblib.dump(model, 'Covid19_model.pkl')
mse = mean_absolute_error(y_train, model.predict(X_train))
print("Training Set Mean Absolute Error: %.4f" % mse)

# Find the error rate on the test set
mse = mean_absolute_error(y_test, model.predict(X_test))
print("Test Set Mean Absolute Error: %.4f" % mse)
